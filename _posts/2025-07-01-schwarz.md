---
layout: post
type: "publication"
title:  "Schwarz-Schur Involution: Lightspeed Differentiable Sparse Linear Solvers"
authors: "Yu Wang, S. Mazdak Abulnaga, Yael Balbastre, Bruce Fischl"
venue: "International Conference on Machine Learning (ICML) 2025"
external-url: "https://people.csail.mit.edu/abulnaga/schwarz_schur/Schwarz_Schur_Involution.pdf"
code: "https://github.com/wangyu9/Schwarz_Schur_Involution"
poster: "https://icml.cc/media/icml-2025/Slides/45286_zk5fglE.pdf"
---

Sparse linear solvers are fundamental to science
and engineering, applied in partial differential
equations (PDEs), scientific computing, computer
vision, and beyond. Indirect solvers possess characteristics that make them undesirable as stable
differentiable modules; existing direct solvers,
though reliable, are too expensive to be adopted in
neural architectures. We substantially acceleratedirect sparse solvers or generalized deconvolution
by up to 3 orders-of-magnitude faster, violating
common assumptions that direct solvers are too
slow. We “condense” a sparse Laplacian matrix
into a dense tensor, a compact data structure that
batch-wise stores the Dirichlet-to-Neumann matrices, reducing the sparse solving to recursively
merging pairs of dense matrices that are much
smaller. The batched small dense systems are
sliced and inverted in parallel to take advantage
of dense GPU BLAS kernels, highly optimized in
the era of deep learning. Our method is efficient,
qualified as a strong zero-shot baseline for AIbased PDE solving and a reliable differentiable
module integrable into machine learning pipelines.
Our code is available at https://github.com/
wangyu9/Schwarz_Schur_Involution.

